import os
import time
import gradio as gr
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

cache_dir = './'
os.system('mkdir /home/xlab-app-center/.cache/model')
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/config.json")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/configuration_internlm.py")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/generation_config.json")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/modeling_internlm.py")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/pytorch_model.bin.index.json")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/special_tokens_map.json")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/tokenization_internlm.py")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/tokenizer.model")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/tokenizer_config.json")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/pytorch_model-00001-of-00002.bin")
os.system("cd /home/xlab-app-center/.cache/model && wget https://download.openxlab.org.cn/models/thomas-yanxin/MindChat-InternLM-7B/weight/pytorch_model-00002-of-00002.bin")

tokenizer = AutoTokenizer.from_pretrained('/home/xlab-app-center/.cache/model',
                                          use_fast=False,
                                          trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("/home/xlab-app-center/.cache/model",
                                             device_map="auto",
                                             torch_dtype=torch.float16,
                                             trust_remote_code=True)
model.generation_config = GenerationConfig.from_pretrained("/home/xlab-app-center/.cache/model")


title = "ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹"

description = """
ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•.

ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡.

ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚

ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
"""
submit_btn = 'å‘é€'
retry_btn = 'ğŸ”„ é‡æ–°ç”Ÿæˆ'
undo_btn = 'â†©ï¸ æ’¤é”€'
clear_btn = 'ğŸ—‘ï¸ æ¸…é™¤å†å²'


def predict(message, history):
    dictionary = {'prompt': message}
    print(dictionary)
    if history is None:
        history = []
    history = history[-3:]
    model_input = []
    for chat in history:
        model_input.append({"role": "user", "content": chat[0]})
        model_input.append({"role": "assistant", "content": chat[1]})
    model_input.append({"role": "user", "content": message})
    print(model_input)
    response = model.chat(tokenizer, model_input)
    print(response)

    history.append((message, response))

    for i in range(len(response)):
        time.sleep(0.02)
        yield response[:i + 1]


demo = gr.ChatInterface(predict,
                        title=title,
                        description=description,
                        cache_examples=True,
                        submit_btn=submit_btn,
                        retry_btn=retry_btn,
                        clear_btn=clear_btn,
                        undo_btn=undo_btn).queue()

if __name__ == "__main__":
    demo.launch()
